{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "}\n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import helpers.algorithms_utils as alg_utils\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from helpers.mv_single_binary import majority_voting\n",
    "from algorithms.smart_stop import decision_function_bayes\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "}\n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vote Adquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_votes_smart_stop(params, items_num, ct, gt, decision_fn):\n",
    "    total_votes = {}\n",
    "    \n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(params['votes_per_item']):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "    #evaluate votes\n",
    "    results = decision_fn(items_num, total_votes, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "    \n",
    "    #Check if must continue collecting votes\n",
    "    items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "    must_get_more_votes = len(items_predicted_classified) > 0\n",
    "     \n",
    "    while(must_get_more_votes):\n",
    "        total_votes_aux = {}\n",
    "        for i, v in items_predicted_classified.items():           \n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "            \n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "            total_votes_aux[i] = total_votes[i].copy()\n",
    "        #end for\n",
    "        \n",
    "        results = decision_fn(len(total_votes_aux), total_votes_aux, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "\n",
    "        #Stop when there are no more items that can be classified\n",
    "        items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "        must_get_more_votes = len(items_predicted_classified) > 0\n",
    "    #end while\n",
    "    \n",
    "    items_classification = alg_utils.classify_items(total_votes, gt, majority_voting, ct)\n",
    "\n",
    "    return items_classification, total_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Runner Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_no_multiprocessing(generate_votes_fn, df, cf, data_true_percentage):\n",
    "    \n",
    "    main_results = []\n",
    "    \n",
    "    for ct in cts:\n",
    "        ct = round(ct, 2) #limit to two decimals\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        precisions = []\n",
    "        costs = []\n",
    "        f_ones = []\n",
    "        f_betas = []\n",
    "        wces = []\n",
    "\n",
    "        for _ in range(iterations_per_ct):\n",
    "            workers_accuracy = alg_utils.simulate_workers(workers_num, z, fixed_acc, fixed_workers_acc, base_workers_acc, max_workers_acc)\n",
    "    \n",
    "            params = {\n",
    "                'workers_accuracy': workers_accuracy,\n",
    "                'workers_num': workers_num,\n",
    "                'items_num': items_num,\n",
    "                'cost_ratio': cr,\n",
    "                'votes_per_item': base_votes_per_item,\n",
    "                'classification_fn': cf\n",
    "            }\n",
    "            \n",
    "            items_classification, total_votes = generate_votes_smart_stop(params, items_num, ct, items_ground_truth, df)\n",
    "            \n",
    "            costs.append(np.mean([len(v) for k,v in total_votes.items()]))\n",
    "\n",
    "            loss, recall, precision, f1, beta, f_beta, wce = alg_utils.Metrics.compute_metrics_full(items_classification, items_ground_truth, fnc, fpc)\n",
    "            losses.append(loss)\n",
    "            recalls.append(recall)\n",
    "            precisions.append(precision)\n",
    "            f_ones.append(f1)\n",
    "            f_betas.append(f_beta)\n",
    "            wces.append(wce)\n",
    "        #end for iterations\n",
    "        \n",
    "        results = [['adaptive_em',\n",
    "                             dataset_name,\n",
    "                             cr,\n",
    "                             str(cf.__name__),\n",
    "                             str(df.__name__),\n",
    "                             ct,\n",
    "                             workers_num,\n",
    "                             f'U({base_workers_acc},{max_workers_acc})',\n",
    "                             items_num,\n",
    "                             data_true_percentage,\n",
    "                             round(np.mean(costs), 3), \n",
    "                             round(np.std(costs), 3),\n",
    "                             round(np.mean(recalls), 3),\n",
    "                             round(np.std(recalls), 3),\n",
    "                             round(np.mean(precisions), 3),\n",
    "                             round(np.std(precisions), 3),           \n",
    "                             round(np.mean(losses), 3),\n",
    "                             round(np.std(losses), 3),                       \n",
    "                             round(np.mean(f_ones), 3),\n",
    "                             round(np.std(f_ones), 3),                        \n",
    "                             round(np.mean(f_betas), 3),\n",
    "                             round(np.std(f_betas), 3),    \n",
    "                             round(np.mean(wces), 3),\n",
    "                             round(np.std(wces), 3),\n",
    "                             fnc,\n",
    "                             fpc,\n",
    "                             ucc\n",
    "                             ]]\n",
    "        pd.DataFrame(results, columns=pdColumns).to_csv(f'{results_file}', mode='a', index=False, header=False)\n",
    "    #endfor cts\n",
    "        \n",
    "    \n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Dataset Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_info(dataset, path):\n",
    "    dataset_name = dataset['name']\n",
    "    dataset_gt = dataset['gt_file']\n",
    "    dataset_votes = dataset['votes_file']\n",
    "    dataset_delimiter = dataset['delimiter']\n",
    "    dataset_header = dataset['header']\n",
    "\n",
    "    gt_index_item = dataset['gt_index_item']\n",
    "    gt_index_value = dataset['gt_index_value']\n",
    "    votes_index_worker = dataset['votes_index_worker']\n",
    "    votes_index_item = dataset['votes_index_item']\n",
    "    votes_index_vote = dataset['votes_index_vote']\n",
    "\n",
    "    gt = pd.read_csv(f'{path}/{dataset_name}/{dataset_gt}', delimiter=dataset_delimiter, header=dataset_header)\n",
    "    votes = pd.read_csv(f'{path}/{dataset_name}/{dataset_votes}', delimiter=dataset_delimiter, header=dataset_header)\n",
    "    \n",
    "    items_num = len(gt[gt_index_item].unique())\n",
    "    votes_num = len(votes)\n",
    "    gt_classes = gt[gt_index_value].unique()\n",
    "    gt_balance = {}\n",
    "\n",
    "    for gt_class in gt_classes:\n",
    "        class_items = gt[gt[gt_index_value] == gt_class]\n",
    "        gt_balance[gt_class] = len(class_items) / items_num\n",
    "        \n",
    "    workers_id = votes[votes_index_worker].unique()\n",
    "\n",
    "    workers_acc = {}\n",
    "    workers_acc_indexed = []\n",
    "    workers_answers_correctness = {}\n",
    "    for worker_id in workers_id:\n",
    "\n",
    "        worker_votes = votes[votes[votes_index_worker] == worker_id]\n",
    "        workers_answers_correctness[worker_id] = []\n",
    "        for key, worker_vote in worker_votes.iterrows():\n",
    "\n",
    "            item_id = worker_vote[votes_index_item]\n",
    "            vote = worker_vote[votes_index_vote]\n",
    "            item_gt = gt[gt[gt_index_item] == item_id][gt_index_value]\n",
    "\n",
    "            if item_gt.empty == False:\n",
    "                workers_answers_correctness[worker_id].append(int(vote == item_gt.iloc[0]))\n",
    "        #end for votes\n",
    "\n",
    "        if len(workers_answers_correctness[worker_id]) != 0:\n",
    "            worker_acc = np.mean(workers_answers_correctness[worker_id])\n",
    "            workers_acc[worker_id] = worker_acc\n",
    "            workers_acc_indexed.append(worker_acc)\n",
    "    #end for workers\n",
    "\n",
    "    avg_acc = np.mean(workers_acc_indexed)\n",
    "    workers_num = len(workers_acc_indexed)\n",
    "    \n",
    "    return items_num, gt_balance, votes_num, workers_num, avg_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "            #{'name':'HITspam', 'gt_file':'gold.txt','votes_file':'labels.txt', 'gt_index_item': 0, 'gt_index_value': 1, 'votes_index_worker': 0, \n",
    "            # 'votes_index_item': 1, 'votes_index_vote': 2, 'delimiter':'\\t', 'header':None},\n",
    "            #{'name':'TEMP', 'gt_file':'gold_and_votes.txt','votes_file':'gold_and_votes.txt', 'gt_index_item': 2, 'gt_index_value': 4, \n",
    "            # 'votes_index_worker': 1, 'votes_index_item': 2, 'votes_index_vote': 3, 'delimiter':'\\t', 'header':None},\n",
    "            {'name':'WVSCM', 'gt_file':'groundTruth.txt','votes_file':'labels.txt', 'gt_index_item': 0, 'gt_index_value': 1, 'votes_index_worker': 0, \n",
    "             'votes_index_item': 1, 'votes_index_vote': 2, 'delimiter':'\\t', 'header':None},  \n",
    "            {'name':'WaterBird1', 'gt_file':'gt.txt','votes_file':'labels.txt', 'gt_index_item': 0, 'gt_index_value': 1, 'votes_index_worker': 0, \n",
    "             'votes_index_item': 1, 'votes_index_vote': 2, 'delimiter':'\\t', 'header':None}, \n",
    "            {'name':'TlkAgg2', 'gt_file':'golden_labels.tsv','votes_file':'crowd_labels.tsv', 'gt_index_item': 0, 'gt_index_value': 1, \n",
    "             'votes_index_worker': 0, 'votes_index_item': 1, 'votes_index_vote': 2, 'delimiter':'\\t', 'header':None},\n",
    "            {'name':'Div150Multi', 'gt_file':'mediaEval1Q1_Gold.txt','votes_file':'mediaEvalQ1_Responses.txt', 'gt_index_item': 0, 'gt_index_value': 1, 'votes_index_worker': 0, \n",
    "             'votes_index_item': 1, 'votes_index_vote': 2, 'delimiter':' ', 'header':None},\n",
    "            {'name':'HC-TREC-2011', 'gt_file':'trec2011Task2_Gold.txt','votes_file':'trec2011Task2_Responses.txt', 'gt_index_item': 0, \n",
    "             'gt_index_value': 1, 'votes_index_worker': 0, 'votes_index_item': 1, 'votes_index_vote': 2, 'delimiter':' ', 'header':None}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdColumns = [\"fn_name\",\"dataset\",\"cost_ratio\", \"class_fn\", \"decision_fn\",\"threshold\",'num_workers',\n",
    "             'workers_distribution','num_items','data_bal','cost','cost_std', 'recall','recall_std', 'precision', \n",
    "             'precision_std', 'loss', 'loss_std', 'f1', 'f1_std', 'fbeta', 'fbeta_std', 'wce','wce_std','fnc','fpc', 'ucc']\n",
    "\n",
    "\n",
    "fnc = 1\n",
    "fpc = 1\n",
    "ucc = 1\n",
    "\n",
    "#cost ratios\n",
    "crs = [1/10, 1/20, 1/50, 1/100]\n",
    "\n",
    "#crowd\n",
    "base_votes_per_item = 3\n",
    "z = 0 #% cheaters\n",
    "\n",
    "fixed_acc = False\n",
    "fixed_workers_acc = None\n",
    "\n",
    "#ground truth \n",
    "#items_num = 1000\n",
    "\n",
    "cts = [.7, .8, .9, .95]\n",
    "iterations_per_ct = 10\n",
    "\n",
    "decision_function = decision_function_bayes\n",
    "cf = majority_voting\n",
    "\n",
    "datasets_path = '/Users/pmaglione/Repos/crowd-datasets'\n",
    "\n",
    "results_file = 'results/adaptive_em/results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_true_positive_percentage(balance):\n",
    "    if 'Yes' in balance.keys():\n",
    "        return balance['Yes']\n",
    "    elif 1 in balance.keys():\n",
    "        return balance[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    items_num, gt_balance, votes_num, workers_num, avg_acc = get_dataset_info(dataset, datasets_path)\n",
    "    \n",
    "    #we only use binary class datasets\n",
    "    if len(gt_balance) == 2:\n",
    "        dataset_name = dataset['name']\n",
    "        base_workers_acc = avg_acc - (1 - avg_acc) / 2\n",
    "        max_workers_acc = avg_acc + (1 - avg_acc) / 2\n",
    "        \n",
    "        workers_num = 2000\n",
    "\n",
    "        data_true_percentage = get_dataset_true_positive_percentage(gt_balance)\n",
    "        items_ground_truth = alg_utils.generate_gold_data(items_num, data_true_percentage)\n",
    "\n",
    "        for cr in crs:\n",
    "            _ = run_experiment_no_multiprocessing(generate_votes_smart_stop, decision_function, cf, data_true_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
