{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Performing an efficient management of budget and securing accuracy are key points when using crowdsourcing. In classification scenarios the crowd sometimes can be noisy and generate a lack of concensus over tasks. The majority of the state-of-art approaches set a fixed number of votes per item and aggregate the votes with some criteria, this produces a reduction in the quality and an increment of costs. For this reason we present a smart detection algorithm which predicts the best decision between stop or continue collecting votes over a task and analyze its efficiency with different experiments.\n",
    "\n",
    "Formalizing the problem, given a set of items $I$, a set of votes $V$, a classification function $fn$, a classification threshold $th$ and a cost ratio for crowd to expert vote cost $cr$, for each item we want to find the minimum amount of votes needed to take the decision of continue collecting votes or switch to an expert vote. For this we describe a smart stopping algorithm. \n",
    "\n",
    "**We define a 3 methods structure**:\n",
    "- the **classifier** which returns the probability of an item being classified\n",
    "- the **cost estimator** which returns the estimated cost for each item given the votes\n",
    "- the **decision function** which returns a boolean decision for each item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rationale for the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: item votes\n",
    "- Output: probability IN [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost estimator function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: item votes, classification threshold, classification function, cost ratio\n",
    "- Output: predicted cost\n",
    "- Logic:\n",
    "    - Iterate N times for converging the results\n",
    "        - Calculate actual cost for the given votes\n",
    "        - While (is not classified or is not too expensive)\n",
    "            - If actual cost is not too expensive\n",
    "                - Get P(IN) using the classification function\n",
    "                - If P(IN) > threshold or P(OUT) > threshold\n",
    "                    - **Stop because item is classified**\n",
    "                - Else\n",
    "                    - Simulate 1 vote over the item using P(IN)\n",
    "                    - Increment actual cost\n",
    "            - Else **stop because the item is too expensive to be classified**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input: items, votes, classification_threshold, cost_ratio, classification_function\n",
    "- Output: Boolean decision over each item, where True = continue and False = stop collecting votes\n",
    "- Logic:\n",
    "    - For each item\n",
    "        - Get P(IN) using the classification function\n",
    "        - If P(IN) > classification threshold or P(OUT) > classification threshold\n",
    "            - Decision over item = False\n",
    "        - Else\n",
    "            - Call **cost estimator function**\n",
    "                - If cost estimated <= max cost\n",
    "                    - Decision over item = True\n",
    "                - Else\n",
    "                    - Decision over item = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Classification without expert vote\n",
    "\n",
    "* The logic for item classification once votes are collected is defined as:\n",
    "    * P(i = OUT|item_votes) = 1 - classifier_function(item_votes)\n",
    "    * If P(i = OUT|item_votes) > classification_threshold -> item is classified as OUT **else** is classified as IN\n",
    "    \n",
    "    \n",
    "    \n",
    "- This can be interpreted as if there isn't enough evidence(votes) to classify the item OUT, is classified IN. This directly affects the Recall metric due to the false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we discuss a few experiments, the objective is to compare the overall crowdsourcing cost and quality in the case where we have a smart stopping algorithm vs \n",
    "- the baseline approach where all items receive the same amount of votes\n",
    "- an approach used by Andrew W. Brown and David B. Allison. 2014. Using Crowdsourcing to Evaluate Published Scientific Literature: Methods and Example. Plos One 9, 7 (2014). Where 2 votes are requested and if they disagree ask a third to break the tie\n",
    "\n",
    "Also we evaluate the performance using a smart stop detection mechanism in terms of cost and quality for balanced and unbalanced datasets, and in different expert costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import helpers.algorithms_utils as alg_utils\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core import display as ICD\n",
    "from helpers.mv_single_binary import majority_voting\n",
    "from helpers.truth_finder import expectation_maximization\n",
    "from helpers.truth_finder_single import truth_finder_single\n",
    "from algorithms.abraham_stop import abraham_stop_binary\n",
    "from algorithms.smart_stop import decision_function_mv\n",
    "from algorithms.smart_stop import decision_function_em\n",
    "from algorithms.smart_stop import decision_function_bayes\n",
    "import multiprocessing\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for all experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Classification function: MV, EM\n",
    " - Cost Ratio: [0,1]\n",
    " - Workers: \n",
    "     - Amount: at least (1/cost_ratio) * increment_parameter\n",
    "     - Percentage of cheaters: [0,1]\n",
    "     - Distribution: [0,1]\n",
    "     - Fixed accuracy for all workers: Boolean\n",
    " - Data:\n",
    "     - Amount: > 0\n",
    "     - Ground Truth Balance(percentage of positive items): [0,1]\n",
    " - Experiment convergence:\n",
    "     - Amount of iterations: > 0\n",
    " - Classification:\n",
    "     - Thresholds: [0,1]\n",
    " - Metrics:\n",
    "     - Loss ratio(penalty for False Negatives): > 0 \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "'''\n",
    "cf = majority_voting\n",
    "cr = .05\n",
    "base_votes_per_item = 3\n",
    "\n",
    "#cost estimator\n",
    "drawing_simulations_amount = 50\n",
    "expert_cost_increment = 2\n",
    "\n",
    "#crowd\n",
    "workers_num = 2000 #fixed number\n",
    "z = 0 #% cheaters\n",
    "base_workers_acc = .5\n",
    "fixed_acc = False\n",
    "fixed_workers_acc = .9\n",
    "\n",
    "\n",
    "#ground truth \n",
    "items_num = 1000\n",
    "data_true_percentage = .5\n",
    "\n",
    "#experiment \n",
    "iterations_per_ct = 50\n",
    "cts = [.5, .6, .7, .8, .9] #classification thresholds\n",
    "\n",
    "#loss ratio: false negative increment penalization\n",
    "lr = 5 #1 means not penalization\n",
    "'''\n",
    "#alg_utils.print_hyperparameters(cf, cr, base_votes_per_item, drawing_simulations_amount, expert_cost_increment, workers_num, z, fixed_acc, base_workers_acc, fixed_workers_acc, items_num, data_true_percentage, iterations_per_ct, cts, lr)\n",
    "\n",
    "pdColumns = [\"loss_ratio\", \"cost_ratio\", \"class_fn\", \"decision_fn\", \"data_bal\", \"threshold\", \"cost\",\"cost_std\", \"loss\", \"loss_std\", \"recall\", \"recall_std\", \"precision\", \"precision_std\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "### Single-predicate classification\n",
    "   - **RTE**: 800 items. \n",
    "     - Includes binary judgments for textual entailment. \n",
    "   - **TEMP**: 462 items. \n",
    "     - Includes binary judgments for temporal ordering (i.e., whether one event follows another).\n",
    "   - **BM**: 1000 items. \n",
    "     - (Mozafari et al. 2012) contains negative/positive sentiment labels assigned by AMT workers to tweets.\n",
    "   - **SpamCF**: 100 items. \n",
    "     - (Ipeirotis 2010) includes binary AMT judgments about whether or not an AMT HIT should be considered a “spam” task, according to their criteria.\n",
    "   - **HCB**: 3275 items. \n",
    "     - Conflates relevant classes toproduce only binary labels (Jung and Lease 2011; 2012). \n",
    "   - **WVSCM**: 159 items. \n",
    "     - (Whitehill et al. 2009) includes AMT binary judgments distinguishing whether or not face images smile.\n",
    "\n",
    "### Multi-predicate classification\n",
    "   - **ohsumed-based screening dataset**: 34k Two-predicate screening datasets presented by medical abstracts on different disease categories. \n",
    "   - **loneliness-slr-2018**: The dataset includes 585 abstracts of papers annotated by an expert for 2 predicates. The papers are from social informatics domain and used as a part of the systematic literature review (SLR). \n",
    "   - **amazon-sentiment-dataset**: 1k-100k dataset on Amazon product reviews, where predicats are 1) Product category 2) Sentiment of the review.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {'folder':'amazon-sentiment-dataset', 'filename':'1k_amazon_reviews_crowdsourced_lemmatized_min3votes.csv','predicates': ['is_negative', 'is_book']},\n",
    "    {'folder':'loneliness-dataset-2018', 'filename':'loneliness-dataset-2018.csv','predicates': ['oa_predicate', 'study_predicate']},\n",
    "    {'folder':'ohsumed_data', 'filename':'ohsumed_C14_C23_1grams.csv','predicates': ['C14', 'C23']},\n",
    "    {'folder':'BarzanMozafari', 'filename':'ground_truth_normalized.csv','predicates':['gt']},\n",
    "    {'folder':'HyunCatherines_Binary', 'filename':'ground_truth_normalized.csv','predicates':['gt']},  \n",
    "    {'folder':'RTE', 'filename':'ground_truth_normalized.csv','predicates':['gt']},\n",
    "    {'folder':'SpamCF', 'filename':'ground_truth_normalized.csv','predicates':['gt']},\n",
    "    {'folder':'TEMP', 'filename':'ground_truth_normalized.csv','predicates':['gt']}, \n",
    "    {'folder':'WVSCM', 'filename':'ground_truth_normalized.csv','predicates':['gt']},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(generate_votes_fn, pool, df, cf, data_true_percentage):\n",
    "    main_results = []\n",
    "    \n",
    "    iterables = []\n",
    "    for ct in cts:\n",
    "        multi = 5\n",
    "        divided_it_per_process = int(iterations_per_ct / multi)\n",
    "        for _ in range(multi):\n",
    "            iterables.append((divided_it_per_process, ct, generate_votes_fn, df, cr, cf, \n",
    "                              workers_num, z, fixed_acc, fixed_workers_acc, base_workers_acc,\n",
    "                              base_votes_per_item, ground_truth, data_true_percentage, items_num\n",
    "                             ))\n",
    "        \n",
    "        \n",
    "        results = pool.map(iterate_ct, iterables)\n",
    "        \n",
    "        main_results.append([lr,\n",
    "                             cr,\n",
    "                             str(cf.__name__),\n",
    "                             str(df.__name__),\n",
    "                             data_true_percentage,\n",
    "                             ct, \n",
    "                             round(np.mean([x[6] for x in results]), 3), \n",
    "                             round(np.mean([x[7] for x in results]), 3),\n",
    "                             round(np.mean([x[8] for x in results]), 3),\n",
    "                             round(np.mean([x[9] for x in results]), 3),\n",
    "                             round(np.mean([x[10] for x in results]), 3),\n",
    "                             round(np.mean([x[11] for x in results]), 3),\n",
    "                             round(np.mean([x[12] for x in results]), 3),\n",
    "                             round(np.mean([x[13] for x in results]), 3)])       \n",
    "        \n",
    "    \n",
    "    return main_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_ct(parameters):\n",
    "        iterations, ct, generate_votes_fn, decision_fn, cr, cf, \\\n",
    "                              workers_num, z, fixed_acc, fixed_workers_acc, base_workers_acc, \\\n",
    "                              base_votes_per_item, ground_truth, data_true_percentage, items_num  = parameters\n",
    "        \n",
    "        ct = round(ct, 2) #limit to two decimals\n",
    "        crowd_cost = []\n",
    "        ct_loss = []\n",
    "        ct_recall = []\n",
    "        ct_precision = []\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            workers_accuracy = alg_utils.simulate_workers(workers_num, z, fixed_acc, fixed_workers_acc, base_workers_acc)\n",
    "            \n",
    "            params = {\n",
    "                'workers_accuracy': workers_accuracy,\n",
    "                'workers_num': workers_num,\n",
    "                'items_num': items_num,\n",
    "                'cost_ratio': cr,\n",
    "                'votes_per_item': base_votes_per_item,\n",
    "                'classification_fn': cf\n",
    "            }\n",
    "    \n",
    "            items_classification, total_votes = generate_votes_fn(params, items_num, ct, ground_truth, decision_fn)\n",
    "            \n",
    "            loss,  recall, precision = alg_utils.Metrics.compute_metrics(items_classification, ground_truth, lr)\n",
    "            ct_loss.append(loss)\n",
    "            ct_recall.append(recall)\n",
    "            ct_precision.append(precision)\n",
    "            \n",
    "            ct_i_crowd_cost = alg_utils.get_total_cost(total_votes, cr)\n",
    "            \n",
    "            crowd_cost.append(ct_i_crowd_cost)\n",
    "        #end for iterations\n",
    "\n",
    "        return [lr,\n",
    "             cr,\n",
    "             str(cf.__name__),\n",
    "             str(decision_fn.__name__),\n",
    "             data_true_percentage,\n",
    "             ct, \n",
    "             round(np.mean(crowd_cost), 3), \n",
    "             round(np.std(crowd_cost), 3),\n",
    "             round(np.mean(ct_loss), 3),\n",
    "             round(np.std(ct_loss), 3),\n",
    "             round(np.mean(ct_recall), 3),\n",
    "             round(np.std(ct_recall), 3),\n",
    "             round(np.mean(ct_precision), 3),\n",
    "             round(np.std(ct_precision), 3)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_no_multiprocessing(generate_votes_fn, pool, df, cf, data_true_percentage):\n",
    "    \n",
    "    main_results = []\n",
    "    \n",
    "    for ct in cts:\n",
    "        ct = round(ct, 2) #limit to two decimals\n",
    "        crowd_cost = []\n",
    "        ct_loss_mv = []\n",
    "        ct_recall_mv = []\n",
    "        ct_precision_mv = []\n",
    "        \n",
    "        for _ in range(iterations_per_ct):\n",
    "            workers_accuracy = alg_utils.simulate_workers(workers_num, z, fixed_acc, fixed_workers_acc, base_workers_acc)\n",
    "            \n",
    "            params = {\n",
    "                'workers_accuracy': workers_accuracy,\n",
    "                'workers_num': workers_num,\n",
    "                'items_num': items_num,\n",
    "                'cost_ratio': cr,\n",
    "                'votes_per_item': base_votes_per_item,\n",
    "                'classification_fn': cf\n",
    "            }\n",
    "    \n",
    "            items_classification, total_votes = generate_votes_fn(params, items_num, ct, ground_truth, df)\n",
    "            \n",
    "            loss,  recall, precision = alg_utils.Metrics.compute_metrics(items_classification, ground_truth, lr)\n",
    "            ct_loss_mv.append(loss)\n",
    "            ct_recall_mv.append(recall)\n",
    "            ct_precision_mv.append(precision)     \n",
    "            \n",
    "            ct_i_crowd_cost = alg_utils.get_total_cost(total_votes, cr)\n",
    "            crowd_cost.append(ct_i_crowd_cost)\n",
    "        #end for iterations\n",
    "\n",
    "        main_results.append([lr,\n",
    "                             cr,\n",
    "                             str(cf.__name__),\n",
    "                             str(df.__name__),\n",
    "                             data_true_percentage,\n",
    "                             ct, \n",
    "                             round(np.mean(crowd_cost), 3), \n",
    "                             round(np.std(crowd_cost), 3),\n",
    "                             round(np.mean(ct_loss_mv), 3),\n",
    "                             round(np.std(ct_loss_mv), 3),\n",
    "                             round(np.mean(ct_recall_mv), 3),\n",
    "                             round(np.std(ct_recall_mv), 3),\n",
    "                             round(np.mean(ct_precision_mv), 3),\n",
    "                             round(np.std(ct_precision_mv), 3)])\n",
    "\n",
    "        \n",
    "    \n",
    "    return main_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Experiments without considering workers accuracy for predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Smart stop with MV as classification function, where the Expected cost is limited by expert cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1) Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Evalute for each classification threshold\n",
    "2. Iterate N times per threshold for converging results doing:\n",
    "    1. Generate workers accuracy\n",
    "    2. Generate ground truth values\n",
    "    3. Generate the base votes for each item\n",
    "    4. Call the decision function\n",
    "        1. If we have any item to continue collecting votes:\n",
    "            1. Collect 1 more vote per item\n",
    "            2. Call decision function\n",
    "    5. Classify items using MV\n",
    "        1. If P(OUT) > threshold, classify it as OUT\n",
    "        2. Else classify it as IN\n",
    "    6. Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_votes_smart_stop(params, items_num, ct, gt, decision_fn):\n",
    "    total_votes = {}\n",
    "    \n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(params['votes_per_item']):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "    #evaluate votes\n",
    "    results = decision_fn(items_num, total_votes, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "    \n",
    "    #Check if must continue collecting votes\n",
    "    items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "    must_get_more_votes = len(items_predicted_classified) > 0\n",
    "     \n",
    "    while(must_get_more_votes):\n",
    "        total_votes_aux = {}\n",
    "        for i, v in items_predicted_classified.items():           \n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "            \n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "            total_votes_aux[i] = total_votes[i].copy()\n",
    "        #end for\n",
    "        \n",
    "        results = decision_fn(len(total_votes_aux), total_votes_aux, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "\n",
    "        #Stop when there are no more items that can be classified\n",
    "        items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "        must_get_more_votes = len(items_predicted_classified) > 0\n",
    "    #end while\n",
    "    \n",
    "    items_classification = alg_utils.classify_items(total_votes, gt, majority_voting, .5)\n",
    "\n",
    "    return items_classification, total_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) MV with fixed number of votes per item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we use the baseline approach of majority voting with a fixed number of votes per item and a classification threshold of .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1) Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iterate N times for converging results where:\n",
    "    1. Generate workers accuracy\n",
    "    2. Generate ground truth values\n",
    "    3. Generate a fixed number of votes for each item\n",
    "    4. Classify items using MV\n",
    "    5. Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_votes_mv_base(params, items_num, ct, gt, decision_fn=None):\n",
    "    total_votes = {}\n",
    "\n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(params['votes_per_item']):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "\n",
    "    items_classification = alg_utils.classify_items(total_votes, gt, params['classification_fn'], ct)\n",
    "    \n",
    "    return items_classification, total_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Two votes first and if disagree one more to break the tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we collect 2 votes per item and a third one if they disagree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.1) Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Iterate N times for converging results where:\n",
    "    1. Generate workers accuracy\n",
    "    2. Generate ground truth values\n",
    "    3. Generate 2 votes for each item\n",
    "    4. Evalutes the votes:\n",
    "        1. If votes agree:\n",
    "            1. Jump to next step\n",
    "        2. If votes disagree:\n",
    "            1. Get 1 more vote \n",
    "    4. Classify items using MV\n",
    "    5. Calculate the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_votes_break_tie(params, items_num, ct, gt, decision_fn=None):\n",
    "    total_votes = {}\n",
    "\n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(2):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "        #if disagree get 1 more vote\n",
    "        votes_disagree = sum([v[0] for i,v in total_votes[i].items()]) == 1\n",
    "        if(votes_disagree):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "\n",
    "    items_classification = alg_utils.classify_items(total_votes, gt, params['classification_fn'], ct)\n",
    "    \n",
    "    return items_classification, total_votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3) Results evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with simulated balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Analysis over different classification thresholds using different expert costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart stop over different expert costs\n",
    "\n",
    "The expert cost has a direct impact in the experiments overall cost, in particular for our approach the expert cost sets a limit for the amount of drawings we do for simulate votes. A lower expert cost means a lower cost ratio between crowd to expert cost, and this is translated in the amount of crowd votes we can collect until reaching the expert.  This is one of the key points in our analysis and this is to choose the best alternative between collecting more votes or not to be more confident for taking a decision for classifying an item. \n",
    "\n",
    "\n",
    "To observe how the expert cost impact in the performance we set different costs: 10, 50, 100 and 500. To clarify the notion this can be read easily as a relation, if our expert cost is 10 and the crowd cost is 1 we have to collect 10 crowd votes to reach the expert cost, and can be seen as a ratio 1:10.\n",
    "\n",
    "to-do clarify:\n",
    "Expert cost -> # crowd votes -> prob -> classification threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SmartStop approach behaviour using MV. Simulations for balanced data, over different classifications threshold = .7, with 1000 balanced items, expert costs 10, 20, 50 and 100, workers acc mean ≈ 76%, loss ratio = 5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graphics we can observe that the cost ratio don't affect the quality. This was expected given the prior analysis of the behaviour of majority voting, when having collected 3 votes which only 2 agree MV gives us a 66,6 probability of being classified as one class, and if we collect 1 more this can turn this probability in 75% if agree and gets classified, or 50% if disagree turning it in what we assume the worse case for drawing because represent random voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart stop over different classification thresholds with fixed expert cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One point to see is the performance of the approach using the smart stop mechanism over different classification thresholds with a fixed expert cost, with this we expect to see the behaviour between the classification function, the threshold and the given cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SmartStop approach behaviour using MV. Simulations for balanced data, over different classifications threshold = [.5, .6, .7, .8, .9], with 1000 balanced items, expert cost 20, workers acc mean ≈ 76%, loss ratio = 5.**\n",
    "\n",
    "### Cost vs loss\n",
    "\n",
    "***SmartStop approach performance analysis:***\n",
    "\n",
    " - We can observe for the classification thresholds .5 and .6 the same behaviour given the percentage obtained by MV when 3 votes are collected. When the 3 votes collected agree MV gives us a 100% probability of being classified as 1 of the 2 classes, either 0 or 1, instead when only 2 votes agree we get a 66,6% chance of classification over one class and this reachs .5 and .6 thresholds acting as the baseline logic. \n",
    "\n",
    " - For threshold .7 we observe that many votes were collected(**1096** more than base) which incremented the quality reducing the loss(and many false negatives) keeping the same precision of lower threshold but incrementing the recall significantly. Comparing the other values we can conclude that this was the better cost-quality performance. \n",
    "\n",
    " - For threshold .8 we can see that more votes have been collected(**738** more than base) but not as many as in the previous, this can be expected due to the bigger difference between the initial item probability of being classified and the threshold(%66,6->%80). The longer this difference is, the more the number of votes will be needed to reach it, and higher is the chance of error. \n",
    "\n",
    " - For threshold .9 no votes were collected and this can be expectable given the big gap between the initial classification probability and threshold(%66,6->%90). We can do further analysis to see this case... **DISCUSS if is needed!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate: penalizing false negatives\n",
    "\n",
    "We add a loss ratio to penalyze the false negatives which we consiser to be more harmful than the false positives, because are those items which are loss forever. With this table we expect to see the performance of the approaches over different classification thresholds, with a loss ratio = 5, expert cost = 20, over 1000 balanced data, 3 initial votes for MV and SMV.\n",
    "\n",
    "We set a loss ratio in 5 as we think is a conservative value.\n",
    "\n",
    "<!---\n",
    "| Approach      | Threshold | Cost   | Error rate | Error rate lr=10 | Precision | Recall |\n",
    "|---------------|-----------|--------|------------|------------------|-----------|--------|\n",
    "| Baseline MV   | .5        | 3000   | 0.156      | 0.874            | 0.845     | 0.842  |\n",
    "| Brown&Allison | .5        | 2377,44| 0.115      | 0.859            | 0.841     | 0.849  |\n",
    "| SmartStop MV  | .5        | 3000   | 0.158      | 0.855            | 0.843     | 0.841  |\n",
    "| SmartStop MV  | .6        | 3000   | 0.157      | 0.852            | 0.843     | 0.843  |\n",
    "| SmartStop MV  | .7        | 4096,16| 0.115      | 0.353            | 0.842     | 0.947  |\n",
    "| SmartStop MV  | .8        | 3738   | 0.241      | 0.321            | 0.679     | 0.983  |\n",
    "| SmartStop MV  | .9        | 3000   | 0.298      | 0.371            | 0.629     | 0.984  |\n",
    "\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance comparison with other approaches\n",
    "\n",
    "In this case we compare the cost and quality results against the other approaches, we can expect to see an improvement in quality at a reasonable cost when the smart stop module predict to collect more votes. Given the conservative classification criteria, which is to collect votes to ***exclude*** items and if there is not enough evidence ***include*** them, affects the **precision** because the increment in **false positives(FP)**, following a binary classification terminology we can describe them as those items which real value is ***0*** and are classified as ***1***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behaviour of the approaches. Simulations for unbalanced data 10% possitives, smart stop evaluated over different classifications threshold = [.5, .6, .7, .8, .9], with 1000 balanced items, expert cost 20, workers acc mean ≈ 76%, loss ratio = 5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the quality was higher when using the module predictions, reducing the loss and incrementing the recall, as an effect of the classification criteria the precision is reduced. In the case of threshold .9 the recall is high because it reduces the false negative amount due to it strictness, but it has a big impact in precision becoming this case the worse one in terms of number of false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2) Experiments with unbalanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we are interested in analyzing the cost-quality performance of the approaches over unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for unbalanced data 10% positives - 90% negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behaviour of the approaches. Simulations for unbalanced data 10% possitives, over different classifications threshold = [.5, .6, .7, .8, .9], with 1000 balanced items, expert cost 20, workers acc mean ≈ 76%, loss ratio = 5.**\n",
    "\n",
    "We can get similar conclusion with unbalanced data, the threshold .7 continue having the better performance in terms of cost-quality. In particular the bigger thresholds(.8 and .9) are affected significantly in this case compared to balanced dataset, again the reason is the impact of false positives that in this case are more likely to appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3) Results for unbalanced data 1% positives - 99% negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behaviour of the approaches. Simulations for unbalanced data 1% possitives, over different classifications threshold = [.5, .6, .7, .8, .9], with 1000 balanced items, expert cost 20, workers acc mean ≈ 76%, loss ratio = 5.**\n",
    "\n",
    "This case is similar to the previous one, with the difference of having a bigger difference in precision values of thresholds .8 and .9, and as expected a directly related loss increment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-world datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon sentiment dataset 1k items unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    {'folder':'amazon-sentiment-dataset', 'filename':'1k_amazon_reviews_crowdsourced_lemmatized_min3votes.csv','predicates':['is_book']},\n",
    "    {'folder':'loneliness-dataset-2018', 'filename':'loneliness-dataset-2018.csv','predicates':['study_predicate']},\n",
    "    {'folder':'ohsumed_data', 'filename':'ohsumed_C14_C23_1grams.csv','predicates':['C14']},\n",
    "]\n",
    "\n",
    "for dataset in datasets:\n",
    "    #work over 1 predicate\n",
    "    base_folder = dataset['folder']\n",
    "    filename = dataset['filename']\n",
    "    path = f'data/{base_folder}/{filename}'\n",
    "    X, y_screening, y_predicate = alg_utils.load_data(path, dataset['predicates'])\n",
    "    ground_truth = y_predicate[dataset['predicates'][0]]\n",
    "    \n",
    "    #analyze max 5k items for now\n",
    "    if (len(ground_truth) > 5000):\n",
    "        ground_truth = ground_truth[:5000]\n",
    "    \n",
    "    items_num = len(ground_truth)\n",
    "\n",
    "    iterations_per_ct = 50\n",
    "    drawing_simulations_amount = 10\n",
    "    lr = 5\n",
    "    cr = 0.05\n",
    "    \n",
    "    cr_str = str(round(cr-int(cr), 2))[2:]\n",
    "    folder_name = f'mv/{base_folder}/it{iterations_per_ct}_d{drawing_simulations_amount}_lr{lr}'\n",
    "    \n",
    "\n",
    "    cts = [.7]\n",
    "\n",
    "    #SMV\n",
    "    results_smv = run_experiment_1()\n",
    "    pd.DataFrame(results_smv, columns=pdColumns) \\\n",
    "    .to_csv(f'results/{folder_name}/smart_stop_mv_not_expert_it{iterations_per_ct}_d{drawing_simulations_amount}_cr{cr_str}.csv', index=False)\n",
    "\n",
    "    \n",
    "    cts = [.5]\n",
    "\n",
    "    #mv baseline\n",
    "    results_baseline = run_experiment_2()\n",
    "    print(pd.DataFrame(results_baseline, columns=pdColumns))\n",
    "        #.to_csv(f'results/{folder_name}/baseline_mv_it{iterations_per_ct}_v{base_votes_per_item}_cr{cr_str}.csv', index=False)\n",
    "\n",
    "\n",
    "    #break tie\n",
    "    results_b_a = run_experiment_3()\n",
    "    pd.DataFrame(results_b_a, columns=pdColumns) \\\n",
    "        .to_csv(f'results/{folder_name}/mv_break_tie_it{iterations_per_ct}_cr{cr_str}.csv', index=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behaviour of the approaches. Simulations for real-world 1k Amazon sentiment data(unbalanced data), expert cost 20, workers acc mean ≈ 76%, loss ratio = 5.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Next step: Experiments estimating workers accuracy, using Expectation Maximization(EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workers: 2000, \n",
      " loss ratio: 5, \n",
      " thresholds: [0.7, 0.8, 0.9, 0.95], \n",
      " accuracy: [0.5,1], \n",
      " iterations per ct: 10\n",
      "decision fn: decision_function_bayes, classification fn: majority_voting, data balance: 0.3, cost ratio: 0.05\n",
      "decision fn: decision_function_bayes, classification fn: majority_voting, data balance: 0.3, cost ratio: 0.1\n",
      "decision fn: decision_function_bayes, classification fn: majority_voting, data balance: 0.1, cost ratio: 0.05\n",
      "decision fn: decision_function_bayes, classification fn: majority_voting, data balance: 0.1, cost ratio: 0.1\n"
     ]
    }
   ],
   "source": [
    "#ground truth data\n",
    "items_num = 1000\n",
    "base_votes_per_item = 3\n",
    "\n",
    "#metrics\n",
    "lr = 5\n",
    "\n",
    "#workers\n",
    "workers_num = 2000 #fixed number\n",
    "z = 0 #% cheaters\n",
    "base_workers_acc = .5\n",
    "fixed_acc = False\n",
    "fixed_workers_acc = .9\n",
    "\n",
    "#iterables\n",
    "cts = [.7 , .8 , .9, .95]\n",
    "iterations_per_ct = 10\n",
    "\n",
    "cost_ratios = [1/20, 1/10]\n",
    "data_true_balances = [.3, .1]\n",
    "classification_fns = [majority_voting]#[majority_voting, truth_finder_single]\n",
    "decision_fns = [decision_function_bayes]#, decision_function_mv]#decision_function_em\n",
    "\n",
    "total_results = []\n",
    "\n",
    "print(f'workers: {workers_num}, \\n loss ratio: {lr}, \\n thresholds: {str(cts)}, \\n accuracy: [{str(base_workers_acc)},1], \\n iterations per ct: {iterations_per_ct}')\n",
    "\n",
    "pool = multiprocessing.Pool(processes=5)\n",
    "\n",
    "for df in decision_fns:\n",
    "    decision_function = df\n",
    "    for cf in classification_fns:\n",
    "        for data_bal in data_true_balances:\n",
    "            data_true_percentage = data_bal\n",
    "            ground_truth = alg_utils.generate_gold_data(items_num, data_true_percentage)\n",
    "            for cr in cost_ratios:\n",
    "                print(f'decision fn: {decision_function.__name__}, classification fn: {cf.__name__}, data balance: {data_bal}, cost ratio: {cr}')\n",
    "                \n",
    "                combination_results = run_experiment_no_multiprocessing(generate_votes_smart_stop, pool, decision_function, cf, data_true_percentage)\n",
    "                for res_a in combination_results:\n",
    "                    total_results.append(res_a)\n",
    "                \n",
    "                pd.DataFrame(total_results, columns=pdColumns).to_csv(f'results/simulated_dataset/all_combinations_results_10_20_unbalanced_bayes.csv', index=False)\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abraham Stopping Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we implement Abraham algorithm for stopping given two parameters $C$ and $e$ which regulates the stopping criteria\n",
    " - Abraham et al., “How Many Workers to Ask?”\n",
    " \n",
    "Logic:\n",
    "Stop of |Va - Vb| >= C*√t - E*t\n",
    "\n",
    "Where t = Va + Vb\n",
    "\n",
    "The paper dont report the best combination of C and E parameters, but shows best performance in $e$= .2 and $e$ = .3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment over a balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abraham_decision(votes, c = 1, e = 1):\n",
    "    return {i:abraham_stop_binary(len([x[0] for k,x in i_votes.items() if x[0] == 1]), len([x[0] for k,x in i_votes.items() if x[0] == 0]), c, e) for i, i_votes in votes.items()}\n",
    "\n",
    "\n",
    "def generate_votes_abraham(params, items_num, ct, gt, df=None):\n",
    "    total_votes = {}\n",
    "    \n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(params['votes_per_item']):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "    #evaluate votes\n",
    "    results = abraham_decision(total_votes, c, e)\n",
    "    \n",
    "    #Check if must continue collecting votes\n",
    "    items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "    must_get_more_votes = len(items_predicted_classified) > 0\n",
    "     \n",
    "    while(must_get_more_votes):\n",
    "        total_votes_aux = {}\n",
    "        for i, v in items_predicted_classified.items():           \n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "            \n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "            total_votes_aux[i] = total_votes[i]\n",
    "        #end for\n",
    "        \n",
    "        results = abraham_decision(total_votes, c, e)\n",
    "\n",
    "        #Stop when there are no more items that can be classified\n",
    "        items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "        must_get_more_votes = len(items_predicted_classified) > 0\n",
    "    #end while\n",
    "    \n",
    "    items_classification = alg_utils.classify_items(total_votes, gt, params['classification_fn'], ct)\n",
    "\n",
    "    return items_classification, total_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [01:22<02:45, 82.61s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [02:46<01:22, 82.94s/it]\u001b[A\n",
      "100%|██████████| 3/3 [04:09<00:00, 82.90s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "lr = 5\n",
    "cr = 1/20\n",
    "ground_truth = alg_utils.generate_gold_data(items_num, data_true_percentage)\n",
    "workers_num = 2000\n",
    "items_num = 1000\n",
    "cts = [.5]\n",
    "cs = [2]\n",
    "es = [.2]\n",
    "iterations_per_ct = 50\n",
    "\n",
    "abraham_columns = [\"c\", \"e\"] + pdColumns\n",
    "data_balances = [.5,.3,.1]\n",
    "decision_function = abraham_decision\n",
    "cf = majority_voting\n",
    "\n",
    "total_results_abraham = []\n",
    "#Abraham\n",
    "for c in cs:\n",
    "    for e in es:\n",
    "        for data_bal in tqdm(data_balances):\n",
    "            data_true_percentage = data_bal\n",
    "\n",
    "            results_abraham = run_experiment_no_multiprocessing(generate_votes_abraham, pool, decision_function, cf, data_true_percentage)\n",
    "            for res_a in results_abraham:\n",
    "                total_results_abraham.append([c, e] + res_a)\n",
    "\n",
    "            pd.DataFrame(total_results_abraham, columns=abraham_columns).to_csv(f'results/abraham/results_c2_e02.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can a observe that a higher $C$ increments quality and cost. For $e$ = .2 we can observe the best performance when $c$ = 1, and when $e$ = .3 the best $C$ is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Multi-Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.sm_run.ShortestMultiRun.helpers.utils import Generator, Workers\n",
    "from algorithms.sm_run.ShortestMultiRun.ShortestMultiRun import ShortestMultiRun\n",
    "\n",
    "'''\n",
    "z - proportion of cheaters\n",
    "lr - loss ration, i.e., how much a False Negative is more harmful than a False Positive\n",
    "votes_per_item - crowd votes per item for base round\n",
    "worker_tests - number of test questions per worker\n",
    "theta - overall proportion of positive items\n",
    "filters_num - number of filters\n",
    "filters_select - selectivity of filters (probability of applying a filter)\n",
    "filters_dif - difficulty of filters\n",
    "iter_num - number of iterations for averaging results\n",
    "---------------------\n",
    "FP == False Exclusion\n",
    "FN == False Inclusion\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "z = 0.3\n",
    "items_per_worker = 10\n",
    "baseround_items = 20  # must be a multiple of items_per_worker\n",
    "if baseround_items % items_per_worker:\n",
    "    raise ValueError('baseround_items must be a multiple of items_per_worker')\n",
    "worker_tests = 5\n",
    "votes_per_item = 3\n",
    "lr = 5\n",
    "theta = 0.3\n",
    "iter_num = 50\n",
    "data = []\n",
    "\n",
    "#filters_num = 2\n",
    "#items_num = 1000\n",
    "#filters_select = [0.14, 0.14, 0.28, 0.42]\n",
    "#filters_dif = [1., 1., 1.1, 0.9]\n",
    "\n",
    "# S-run algorithm\n",
    "loss_smrun_list = []\n",
    "cost_smrun_list = []\n",
    "rec_sm, pre_sm, f_sm, f_sm = [], [], [], []\n",
    "\n",
    "filters_dif = []\n",
    "\n",
    "#gt\n",
    "# 1 = loneliness\n",
    "for i in range(len(datasets)):\n",
    "    \n",
    "    filters_num = len(datasets[i]['predicates'])\n",
    "    \n",
    "    \n",
    "    if (len(datasets[i]['predicates']) == 1):\n",
    "        predicate_type = 'single'\n",
    "        ground_truth = alg_utils.load_data(datasets, i, predicate_type)\n",
    "        gt_p1 = ground_truth[datasets[i]['predicates'][0]]\n",
    "        filters_select = [\n",
    "            sum(gt_p1) / len(gt_p1)\n",
    "        ]\n",
    "    else:\n",
    "        predicate_type = 'multi'\n",
    "        ground_truth = alg_utils.load_data(datasets, i, predicate_type)\n",
    "        gt_p1 = ground_truth[datasets[i]['predicates'][0]]\n",
    "        gt_p2 = ground_truth[datasets[i]['predicates'][1]]\n",
    "        filters_select = [\n",
    "            sum(gt_p1) / len(gt_p1),\n",
    "            sum(gt_p2) / len(gt_p2)\n",
    "        ] \n",
    "    \n",
    "    \n",
    "    items_num = len(ground_truth[datasets[i]['predicates'][0]])\n",
    "\n",
    "    params = {\n",
    "        'filters_num': filters_num,\n",
    "        'items_num': items_num,\n",
    "        'baseround_items': baseround_items,\n",
    "        'items_per_worker': items_per_worker,\n",
    "        'votes_per_item': votes_per_item,\n",
    "        'filters_select': filters_select,\n",
    "        'filters_dif': filters_dif,\n",
    "        'worker_tests': worker_tests,\n",
    "        'lr': lr,\n",
    "        'stop_score': 100\n",
    "    }\n",
    "\n",
    "    for _ in range(iter_num):\n",
    "        # quiz, generation votes\n",
    "        workers_accuracy = Workers(worker_tests, z).simulate_workers()\n",
    "        params.update({'workers_accuracy': workers_accuracy,\n",
    "                       'ground_truth': None\n",
    "                       })\n",
    "\n",
    "        _, ground_truth = Generator(params).generate_votes_gt(items_num)\n",
    "        params.update({'ground_truth': ground_truth})\n",
    "\n",
    "        # s-run\n",
    "        loss_smrun, cost_smrun, rec_sm_, pre_sm_, f_beta_sm = ShortestMultiRun(params).run()\n",
    "        loss_smrun_list.append(loss_smrun)\n",
    "        cost_smrun_list.append(cost_smrun)\n",
    "        rec_sm.append(rec_sm_)\n",
    "        pre_sm.append(pre_sm_)\n",
    "        f_sm.append(f_beta_sm)\n",
    "\n",
    "    data.append([worker_tests, worker_tests, lr, np.mean(loss_smrun_list), np.std(loss_smrun_list),\n",
    "                 np.mean(cost_smrun_list), np.std(cost_smrun_list), 'Crowd-Ensemble', np.mean(rec_sm),\n",
    "                 np.std(rec_sm), np.mean(pre_sm), np.std(pre_sm), np.mean(f_sm), np.std(f_sm),\n",
    "                 baseround_items, items_num, theta, filters_num])\n",
    "\n",
    "    print('SM-RUN    loss: {:1.3f}, loss_std: {:1.3f}, recall: {:1.2f}, rec_std: {:1.3f}, '\n",
    "          'price: {:1.2f}, price_std: {:1.2f}, precision: {:1.3f}, f_b: {}'\n",
    "          .format(np.mean(loss_smrun_list), np.std(loss_smrun_list), np.mean(rec_sm),\n",
    "                  np.std(rec_sm), np.mean(cost_smrun_list), np.std(cost_smrun_list),\n",
    "                  np.mean(pre_sm), np.mean(f_sm)))\n",
    "\n",
    "    dataset_name = datasets[i]['folder']\n",
    "    pd.DataFrame(data,\n",
    "                 columns=['worker_tests', 'worker_tests', 'lr', 'loss_mean', 'loss_std', 'price_mean', 'price_std',\n",
    "                          'algorithm', 'recall', 'recall_std', 'precision', 'precision_std',\n",
    "                          'f_beta', 'f_beta_std', 'baseround_items', 'total_items', 'theta', 'filters_num']\n",
    "                 ).to_csv(f'results/sm_run/{dataset_name}_base.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM-RUN    loss: 0.271, loss_std: 0.067, recall: 0.73, rec_std: 0.146, price: 11.35, price_std: 1.05, precision: 0.971, f_b: 0.7570017095420997\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from algorithms.sm_run.ShortestMultiRun.helpers.utils import Generator, Workers\n",
    "from algorithms.sm_run.ShortestMultiRun.ShortestMultiRun import ShortestMultiRun\n",
    "\n",
    "'''\n",
    "z - proportion of cheaters\n",
    "lr - loss ration, i.e., how much a False Negative is more harmful than a False Positive\n",
    "votes_per_item - crowd votes per item for base round\n",
    "worker_tests - number of test questions per worker\n",
    "theta - overall proportion of positive items\n",
    "filters_num - number of filters\n",
    "filters_select - selectivity of filters (probability of applying a filter)\n",
    "filters_dif - difficulty of filters\n",
    "iter_num - number of iterations for averaging results\n",
    "---------------------\n",
    "FP == False Exclusion\n",
    "FN == False Inclusion\n",
    "'''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    z = 0.3\n",
    "    items_num = 1000\n",
    "    items_per_worker = 10\n",
    "    baseround_items = 20  # must be a multiple of items_per_worker\n",
    "    if baseround_items % items_per_worker:\n",
    "        raise ValueError('baseround_items must be a multiple of items_per_worker')\n",
    "    worker_tests = 5\n",
    "    votes_per_item = 3\n",
    "    lr = 5\n",
    "    filters_num = 4\n",
    "    theta = 0.3\n",
    "    filters_select = [0.14, 0.14, 0.28, 0.42]\n",
    "    filters_dif = [1., 1., 1.1, 0.9]\n",
    "    iter_num = 50\n",
    "    data = []\n",
    "    \n",
    "    params = {\n",
    "        'filters_num': filters_num,\n",
    "        'items_num': items_num,\n",
    "        'baseround_items': baseround_items,\n",
    "        'items_per_worker': items_per_worker,\n",
    "        'votes_per_item': votes_per_item,\n",
    "        'filters_select': filters_select,\n",
    "        'filters_dif': filters_dif,\n",
    "        'worker_tests': worker_tests,\n",
    "        'lr': lr,\n",
    "        'stop_score': 100\n",
    "    }\n",
    "\n",
    "    # S-run algorithm\n",
    "    loss_smrun_list = []\n",
    "    cost_smrun_list = []\n",
    "    rec_sm, pre_sm, f_sm, f_sm = [], [], [], []\n",
    "    for _ in range(iter_num):\n",
    "        # quiz, generation votes\n",
    "        workers_accuracy = Workers(worker_tests, z).simulate_workers()\n",
    "        params.update({'workers_accuracy': workers_accuracy,\n",
    "                       'ground_truth': None\n",
    "                       })\n",
    "\n",
    "        _, ground_truth = Generator(params).generate_votes_gt(items_num)\n",
    "        params.update({'ground_truth': ground_truth})\n",
    "\n",
    "        # s-run\n",
    "        loss_smrun, cost_smrun, rec_sm_, pre_sm_, f_beta_sm = ShortestMultiRun(params).run()\n",
    "        loss_smrun_list.append(loss_smrun)\n",
    "        cost_smrun_list.append(cost_smrun)\n",
    "        rec_sm.append(rec_sm_)\n",
    "        pre_sm.append(pre_sm_)\n",
    "        f_sm.append(f_beta_sm)\n",
    "\n",
    "    data.append([worker_tests, worker_tests, lr, np.mean(loss_smrun_list), np.std(loss_smrun_list),\n",
    "                 np.mean(cost_smrun_list), np.std(cost_smrun_list), 'Crowd-Ensemble', np.mean(rec_sm),\n",
    "                 np.std(rec_sm), np.mean(pre_sm), np.std(pre_sm), np.mean(f_sm), np.std(f_sm),\n",
    "                 baseround_items, items_num, theta, filters_num])\n",
    "\n",
    "    print('SM-RUN    loss: {:1.3f}, loss_std: {:1.3f}, recall: {:1.2f}, rec_std: {:1.3f}, '\n",
    "          'price: {:1.2f}, price_std: {:1.2f}, precision: {:1.3f}, f_b: {}'\n",
    "          .format(np.mean(loss_smrun_list), np.std(loss_smrun_list), np.mean(rec_sm),\n",
    "                  np.std(rec_sm), np.mean(cost_smrun_list), np.std(cost_smrun_list),\n",
    "                  np.mean(pre_sm), np.mean(f_sm)))\n",
    "\n",
    "    pd.DataFrame(data,\n",
    "                 columns=['worker_tests', 'worker_tests', 'lr', 'loss_mean', 'loss_std', 'price_mean', 'price_std',\n",
    "                          'algorithm', 'recall', 'recall_std', 'precision', 'precision_std',\n",
    "                          'f_beta', 'f_beta_std', 'baseround_items', 'total_items', 'theta', 'filters_num']\n",
    "                 ).to_csv('output_smv.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Another Label? Sheng et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def gal_uncertainty(a, b, x = .5):\n",
    "    result = 1\n",
    "    a = a + 1\n",
    "    b = b + 1\n",
    "    for j in np.arange(1, a + b, 1):\n",
    "        print(j)\n",
    "        term_1 = math.factorial(a + b - 1) / (math.factorial(j) * math.factorial(a + b - 1 - j))\n",
    "        term_2 = math.pow(x, j)\n",
    "        term_3 = math.pow(1 - x, a + b - 1 - j)\n",
    "\n",
    "        result *= term_1 * term_2 * term_3\n",
    "        \n",
    "    return result\n",
    "    \n",
    "gal_uncertainty(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import binom\n",
    "\n",
    "pos_c = 1\n",
    "neg_c = 3\n",
    "n = 1\n",
    "filter_acc = .7\n",
    "filter_select = .8\n",
    "term_neg = binom(pos_c + neg_c + n, neg_c + n) * filter_acc ** (neg_c + n) \\\n",
    "           * (1 - filter_acc) ** pos_c * filter_select\n",
    "term_pos = binom(pos_c + neg_c + n, pos_c) * filter_acc ** pos_c \\\n",
    "           * (1 - filter_acc) ** (neg_c + n) * (1 - filter_select)\n",
    "prob_item_neg = term_neg / (term_neg + term_pos)\n",
    "print(prob_item_neg)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0.5, 0.6, 0.7, 0.8, 0.9]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str([.5, .6, .7 , .8 , .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
