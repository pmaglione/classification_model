{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Given a set of items $I$, a set of votes $V$, a classification function $fn$, a classification threshold $th$ and a cost ratio for crowd to expert vote cost $cr$, for each item we want to find the minimum amount of votes needed to take the decision of continue collecting votes or swith to an expert vote. For this we describe a smart stopping algorithm. <br>\n",
    "**We define a 3 methods structure**:\n",
    "- the **classification fn** which returns the probability of an item being classified\n",
    "- the **cost estimator** which returns the estimated cost for each item given the votes\n",
    "- the **decision function** which returns a boolean decision for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "#import cf's libs\n",
    "from helpers.mv_single_binary import majority_voting\n",
    "from helpers.algorithms_utils import input_adapter_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rationale for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max 20 lines\n",
    "'''\n",
    "    Input:\n",
    "        v - votes for item i\n",
    "        ct - value between 0 and 1 for deciding if prob of data is enough or must continue\n",
    "        cf - function to calculate how likely is to be classified\n",
    "        cr - cost ratio between crowd to expert vote [0,1]\n",
    "    Output:\n",
    "        (cost_mean, cost_std)\n",
    "'''\n",
    "def cost_estimator(v, ct, cf, cr):\n",
    "    actual_cost, expert_cost, must_continue = len(v) * cr, 1/cr, True\n",
    "    simulated_costs = []\n",
    "    for _ in range(drawing_simulations_amount):            \n",
    "        while (must_continue == True):\n",
    "            classification_prob = cf(input_adapter_single(v))\n",
    "            if classification_prob > ct:\n",
    "                must_continue = False            \n",
    "                simulated_costs.append(actual_cost)\n",
    "            else:\n",
    "                vote = np.random.binomial(1, classification_prob)\n",
    "                new_index = max(v.keys()) + 1\n",
    "                v[new_index] = [vote]\n",
    "                actual_cost += 1\n",
    "                if(actual_cost >= (expert_cost * expert_cost_increment)):\n",
    "                    must_continue = False\n",
    "                    simulated_costs.append(actual_cost)\n",
    "\n",
    "    return (np.mean(simulated_costs),np.std(simulated_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max 20 lines\n",
    "'''\n",
    "Function to answer: must continue collecting votes over each task?\n",
    "\n",
    "Input:\n",
    "items - set of items\n",
    "votes - set of votes over each item\n",
    "classification_threshold - value between 0 and 1 for deciding if prob of data is enough or must continue\n",
    "cost_ratio - ratio of crowd to expert cost, [0,1]\n",
    "classification_function - function to calculate how likely is to be classified\n",
    "\n",
    "Output:\n",
    "    Dictionary with the decision indexed by item_id\n",
    "        {\n",
    "            item_id: bool\n",
    "            ...\n",
    "            item_n: ...\n",
    "        }\n",
    "    Where False = Stop and True=Continue collecting votes\n",
    "'''\n",
    "def decision_function(items, votes, classification_threshold, cost_ratio, classification_function):      \n",
    "    expert_cost = 1 / cost_ratio  \n",
    "    results = dict.fromkeys(range(items), False)\n",
    "\n",
    "    for item_id in range(items):            \n",
    "        item_votes = votes[item_id].copy()\n",
    "        actual_cost = len(item_votes) \n",
    "        classification_prob = classification_function(input_adapter_single(item_votes))\n",
    "        if classification_prob <= classification_threshold:\n",
    "            cost_mean, cost_std = cost_estimator(item_votes, classification_threshold, classification_function, cost_ratio)\n",
    "\n",
    "            if(cost_mean <= expert_cost):\n",
    "                results[item_id] = True\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we discuss a few experiments, the objective is to compare the overall crowdsourcing cost and quality in the case where we have a smart stopping algorithm vs \n",
    "- the baseline approach where all items receive the same amount of votes\n",
    "- an xx approach where you ask like 2 votes, if they disagree ask a third to break the tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic settings: MV as classification function, Expected cost limited by expert cost\n",
    "\n",
    "**Assumptions**:\n",
    "- The items are evaluated over only 1 condition\n",
    "- Difficulty of tasks are all equal\n",
    "- There are no test questions\n",
    "- The crowd workers accuracy is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "#main \n",
    "cf = majority_voting\n",
    "ct = .9\n",
    "cr = .01 #ratio 1:100\n",
    "base_votes_per_item = 3\n",
    "\n",
    "#cost estimator \n",
    "drawing_simulations_amount = 50\n",
    "expert_cost_increment = 2\n",
    "\n",
    "#crowd\n",
    "workers_num = 1000\n",
    "z = 0 #% cheaters\n",
    "fixed_acc = True\n",
    "workers_acc = .9\n",
    "\n",
    "#ground truth \n",
    "items_num = 100\n",
    "data_true_percentage = 1\n",
    "\n",
    "#experiment \n",
    "cts = np.arange(.7, .96, 0.05) #classification thresholds\n",
    "iterations_per_ct = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate workers\n",
    "def simulate_workers(workers_num, cheaters_prop, fixed_acc, workers_acc):\n",
    "    workers = []\n",
    "    for _ in range(workers_num):\n",
    "        if (fixed_acc == False):\n",
    "            if np.random.binomial(1, cheaters_prop):\n",
    "                # worker_type is 'rand_ch'\n",
    "                worker_acc_pos = worker_acc_neg = 0.5\n",
    "            else:\n",
    "                # worker_type is 'worker'\n",
    "                worker_acc_pos = 0.5 + (np.random.beta(1, 1) * 0.5)\n",
    "                worker_acc_neg = worker_acc_pos + 0.1 if worker_acc_pos + 0.1 <= 1. else 1.\n",
    "        else:\n",
    "            worker_acc_pos = workers_acc\n",
    "            worker_acc_neg = worker_acc_pos + 0.1 if worker_acc_pos + 0.1 <= 1. else 1.\n",
    "\n",
    "        workers.append([worker_acc_pos, worker_acc_neg])\n",
    "\n",
    "    return workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    items_num - number of items\n",
    "    possitive_percentage - [0,1] percentage of possitive items\n",
    "'''\n",
    "def generate_gold_data(items_num, possitive_percentage):\n",
    "    pos_items_number = int(round(((possitive_percentage * 100) * items_num) / 100))     \n",
    "    gold_data = ([1] * pos_items_number) + ([0] * (items_num - pos_items_number))\n",
    "    random.shuffle(gold_data)\n",
    "\n",
    "    return gold_data\n",
    "\n",
    "def classify_items(votes, gt, cf, th):\n",
    "    items_classification = {}\n",
    "    for i, v in votes.items():\n",
    "        prob = cf(input_adapter_single(v))\n",
    "        if (prob > th):\n",
    "            if(gt[i] == 1):\n",
    "                items_classification[i] = 1\n",
    "            else:\n",
    "                items_classification[i] = 0\n",
    "        else:\n",
    "            items_classification[i] = gt[i] #if didnt reach the threshold, switch to expert vote\n",
    "\n",
    "    return items_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(items_classification, gt):\n",
    "        gt_scope = []\n",
    "\n",
    "        # FP == False Inclusion\n",
    "        # FN == False Exclusion\n",
    "        fp = fn = tp = tn = 0.\n",
    "        for item_id in range(len(gt)):\n",
    "            gt_val = gt[item_id]\n",
    "            cl_val = items_classification[item_id]\n",
    "            if gt_val and not cl_val:\n",
    "                fp += 1\n",
    "            if not gt_val and cl_val:\n",
    "                fn += 1\n",
    "            if gt_val and cl_val:\n",
    "                tn += 1\n",
    "            if not gt_val and not cl_val:\n",
    "                tp += 1\n",
    "        if (tp + fn > 0):\n",
    "            recall = tp / (tp + fn)\n",
    "            precision = tp / (tp + fp)\n",
    "        else:\n",
    "            recall = tp\n",
    "            precision = tp\n",
    "        loss = (fp + fn) / len(gt)\n",
    "        \n",
    "        return loss,  recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.workers_accuracy = params['workers_accuracy']\n",
    "        self.workers_num = params['workers_num']      \n",
    "        self.items_num = params['items_num']      \n",
    "        self.cost_ratio = params.get('cost_ratio')\n",
    "        self.classification_threshold = params.get('classification_threshold')\n",
    "        self.index_workers_voted_on_item = {}\n",
    "        self.votes_per_item = params['votes_per_item']\n",
    "        self.classification_fn = params['classification_fn']\n",
    "    \n",
    "    def get_random_worker_accuracy(self, item, items_num):\n",
    "        #TO-DO\n",
    "        '''\n",
    "        worker_found = False  \n",
    "        while (worker_found == False):\n",
    "            worker_id = random.randint(0, self.workers_num - 1)\n",
    "            if (item in self.index_workers_voted_on_item.keys()):\n",
    "                if (worker_id not in self.index_workers_voted_on_item[item]):\n",
    "                    self.index_workers_voted_on_item[item].append(worker_id)\n",
    "                    worker_found = True\n",
    "            else:\n",
    "                worker_found = True\n",
    "                self.index_workers_voted_on_item[item] = [worker_id]\n",
    "        '''\n",
    "       \n",
    "        worker_id = random.randint(0, self.workers_num - 1)\n",
    "        worker_acc_pos = self.workers_accuracy[worker_id][0]\n",
    "        worker_acc_neg = self.workers_accuracy[worker_id][1]\n",
    "        return {'worker_id': worker_id, 'acc_pos':worker_acc_pos, 'acc_neg': worker_acc_neg}\n",
    "    \n",
    "    def get_worker_vote(self, i, gt, items_num):\n",
    "        worker_data = self.get_random_worker_accuracy(i, items_num)\n",
    "        worker_id, worker_acc_pos, worker_acc_neg = worker_data['worker_id'], worker_data['acc_pos'], worker_data['acc_neg']\n",
    "        \n",
    "        item_is_pos = gt[i] == 1\n",
    "        \n",
    "        if (item_is_pos):\n",
    "            worker_acc = worker_acc_pos\n",
    "        else:\n",
    "            worker_acc = worker_acc_neg\n",
    "        \n",
    "        if np.random.binomial(1, worker_acc):\n",
    "            vote = gt[i]\n",
    "        else:\n",
    "            vote = 1 - gt[i]\n",
    "            \n",
    "        return (worker_id, vote)\n",
    "    \n",
    "    def get_items_predicted_classified(results):\n",
    "        return {i:v for (i,v) in results.items() if v == True}\n",
    "    \n",
    "    def generate_votes(self, items_num, ct, gt):\n",
    "        total_votes = {}\n",
    "     \n",
    "        #base votes\n",
    "        for i in range(items_num):\n",
    "            total_votes[i] = {}\n",
    "            for k in range(self.votes_per_item):\n",
    "                worker_id, vote = self.get_worker_vote(i, gt, items_num)\n",
    "\n",
    "                total_votes[i][worker_id] = [vote]\n",
    "                \n",
    "        #evaluate votes\n",
    "   \n",
    "        results = decision_function(items_num, total_votes, ct, self.cost_ratio, \n",
    "                                                       self.classification_fn)\n",
    "        #Check if must continue collecting votes\n",
    "        items_predicted_classified = Generator.get_items_predicted_classified(results)\n",
    "        must_get_more_votes = len(items_predicted_classified) > 0\n",
    "\n",
    "        while(must_get_more_votes):\n",
    "            for i, v in items_predicted_classified.items():\n",
    "                worker_id, vote = self.get_worker_vote(i, gt, items_num)\n",
    "\n",
    "                total_votes[i][worker_id] = [vote]             \n",
    "            #end for\n",
    "            results = decision_function(items_num, total_votes, ct, self.cost_ratio, \n",
    "                                                       self.classification_fn)\n",
    "            \n",
    "            #Stop when there are no more items that can be classified\n",
    "            items_predicted_classified = Generator.get_items_predicted_classified(results)\n",
    "            must_get_more_votes = len(items_predicted_classified) > 0\n",
    "        #end while\n",
    "        \n",
    "        items_classification = classify_items(total_votes, gt, self.classification_fn, ct)\n",
    "            \n",
    "        return [items_classification, total_votes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [13:43<00:00, 196.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Cost Avg</th>\n",
       "      <th>Cost Std</th>\n",
       "      <th>Cost per item</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Loss Std</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Recall Std</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Precision Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.70</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.75</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.85</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.95</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold  Cost Avg  Cost Std  Cost per item  Loss  Loss Std  Recall  \\\n",
       "0       0.70       3.0       0.0           0.03   0.0       0.0     0.0   \n",
       "1       0.75       3.0       0.0           0.03   0.0       0.0     0.0   \n",
       "2       0.80       3.0       0.0           0.03   0.0       0.0     0.0   \n",
       "3       0.85       3.0       0.0           0.03   0.0       0.0     0.0   \n",
       "4       0.90       3.0       0.0           0.03   0.0       0.0     0.0   \n",
       "5       0.95       3.0       0.0           0.03   0.0       0.0     0.0   \n",
       "\n",
       "   Recall Std  Precision  Precision Std  \n",
       "0         0.0        0.0            0.0  \n",
       "1         0.0        0.0            0.0  \n",
       "2         0.0        0.0            0.0  \n",
       "3         0.0        0.0            0.0  \n",
       "4         0.0        0.0            0.0  \n",
       "5         0.0        0.0            0.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_results = []\n",
    "\n",
    "workers_accuracy = simulate_workers(workers_num, z, fixed_acc, workers_acc)\n",
    "\n",
    "params = {\n",
    "    'workers_accuracy': workers_accuracy,\n",
    "    'workers_num': workers_num,\n",
    "    'items_num': items_num,\n",
    "    'cost_ratio': cr,\n",
    "    'votes_per_item': base_votes_per_item,\n",
    "    'classification_fn': cf\n",
    "}\n",
    "\n",
    "for ct in tqdm(cts):\n",
    "    ct = round(ct, 2) #limit to two decimals\n",
    "    crowd_cost = []\n",
    "    items_classified_in = []\n",
    "    items_classified_out = []\n",
    "    ct_loss = []\n",
    "    ct_recall = []\n",
    "    ct_precision = []\n",
    "    \n",
    "    for _ in range(iterations_per_ct):\n",
    "        ground_truth = generate_gold_data(items_num, data_true_percentage)\n",
    "       \n",
    "        ct_i_results = Generator(params).generate_votes(items_num, ct, ground_truth)\n",
    "        \n",
    "        items_classification = ct_i_results[1]\n",
    "        total_votes = ct_i_results[1]\n",
    "\n",
    "        crowd_cost.append(sum([len(v) for (x,v) in th_total_votes.items()]) * cr)\n",
    "        \n",
    "        loss,  recall, precision = Metrics.compute_metrics(items_classification, ground_truth)\n",
    "        ct_loss.append(loss)\n",
    "        ct_recall.append(recall)\n",
    "        ct_precision.append(precision)\n",
    "    #end for iterations\n",
    "    \n",
    "    main_results.append(\n",
    "        [ct, round(np.mean(crowd_cost), 3), \n",
    "         round(np.std(crowd_cost), 3), \n",
    "         round(np.mean(crowd_cost) / items_num, 3),\n",
    "         np.mean(ct_loss),\n",
    "         np.std(ct_loss),\n",
    "         np.mean(ct_recall),\n",
    "         np.std(ct_recall),\n",
    "         np.mean(ct_precision),\n",
    "         np.std(ct_precision)\n",
    "        ])\n",
    "#end for thresholds\n",
    "\n",
    "pd.DataFrame(main_results, columns=[\"Threshold\",\"Cost Avg\",\"Cost Std\", \"Cost per item\", \"Loss\", \"Loss Std\", \"Recall\", \"Recall Std\", \"Precision\", \"Precision Std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
