{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "Given a set of items $I$, a set of votes $V$, a classification function $fn$, a classification threshold $th$ and a cost ratio for crowd to expert vote cost $cr$, for each item we want to find the minimum amount of votes needed to take the decision of continue collecting votes or swith to an expert vote. For this we describe a smart stopping algorithm. <br>\n",
    "**We define a 3 methods structure**:\n",
    "- the **classification fn** which returns the probability of an item being classified\n",
    "- the **cost estimator** which returns the estimated cost for each item given the votes\n",
    "- the **decision function** which returns a boolean decision for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "#import cf's libs\n",
    "from helpers.mv_single_binary import majority_voting\n",
    "import helpers.algorithms_utils as alg_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rationale for the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max 20 lines\n",
    "'''\n",
    "    Input:\n",
    "        v - votes for item i\n",
    "        ct - value between 0 and 1 for deciding if prob of data is enough or must continue\n",
    "        cf - function to calculate how likely is to be classified\n",
    "        cr - cost ratio between crowd to expert vote [0,1]\n",
    "    Output:\n",
    "        (cost_mean, cost_std)\n",
    "'''\n",
    "def cost_estimator(v, ct, cf, cr):\n",
    "    actual_cost, expert_cost, must_continue = len(v) * cr, 1/cr, True\n",
    "    simulated_costs = []\n",
    "    for _ in range(drawing_simulations_amount):            \n",
    "        while (must_continue == True):\n",
    "            classification_prob = cf(input_adapter_single(v))\n",
    "            if classification_prob > ct:\n",
    "                must_continue = False            \n",
    "                simulated_costs.append(actual_cost)\n",
    "            else:\n",
    "                vote = np.random.binomial(1, classification_prob)\n",
    "                new_index = max(v.keys()) + 1\n",
    "                v[new_index] = [vote]\n",
    "                actual_cost += 1\n",
    "                if(actual_cost >= (expert_cost * expert_cost_increment)):\n",
    "                    must_continue = False\n",
    "                    simulated_costs.append(actual_cost)\n",
    "\n",
    "    return (np.mean(simulated_costs),np.std(simulated_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max 20 lines\n",
    "'''\n",
    "Function to answer: must continue collecting votes over each task?\n",
    "\n",
    "Input:\n",
    "items - set of items\n",
    "votes - set of votes over each item\n",
    "classification_threshold - value between 0 and 1 for deciding if prob of data is enough or must continue\n",
    "cost_ratio - ratio of crowd to expert cost, [0,1]\n",
    "classification_function - function to calculate how likely is to be classified\n",
    "\n",
    "Output:\n",
    "    Dictionary with the decision indexed by item_id\n",
    "        {\n",
    "            item_id: bool\n",
    "            ...\n",
    "            item_n: ...\n",
    "        }\n",
    "    Where False = Stop and True=Continue collecting votes\n",
    "'''\n",
    "def decision_function(items, votes, classification_threshold, cost_ratio, classification_function):      \n",
    "    expert_cost = 1 / cost_ratio  \n",
    "    results = dict.fromkeys(range(items), False)\n",
    "\n",
    "    for item_id in range(items):            \n",
    "        item_votes = votes[item_id].copy()\n",
    "        actual_cost = len(item_votes) \n",
    "        classification_prob = classification_function(input_adapter_single(item_votes))\n",
    "        if classification_prob <= classification_threshold:\n",
    "            cost_mean, cost_std = cost_estimator(item_votes, classification_threshold, classification_function, cost_ratio)\n",
    "\n",
    "            if(cost_mean <= expert_cost):\n",
    "                results[item_id] = True\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we discuss a few experiments, the objective is to compare the overall crowdsourcing cost and quality in the case where we have a smart stopping algorithm vs \n",
    "- the baseline approach where all items receive the same amount of votes\n",
    "- an xx approach where you ask like 2 votes, if they disagree ask a third to break the tie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All experiments must call this function and show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Basic settings: MV as classification function, Expected cost limited by expert cost\n",
    "\n",
    "**Assumptions**:\n",
    "- The items are evaluated over only 1 condition\n",
    "- Difficulty of tasks are all equal\n",
    "- There are no test questions\n",
    "- The crowd workers accuracy is fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_votes(params, items_num, ct, gt):\n",
    "    total_votes = {}\n",
    "\n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(params['votes_per_item']):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes, items_num)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "\n",
    "    #evaluate votes\n",
    "    results = decision_function(items_num, total_votes, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "    #Check if must continue collecting votes\n",
    "    items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "    must_get_more_votes = len(items_predicted_classified) > 0\n",
    "\n",
    "    while(must_get_more_votes):\n",
    "        for i, v in items_predicted_classified.items():\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes, items_num)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]             \n",
    "        #end for\n",
    "        results = decision_function(items_num, total_votes, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "\n",
    "        #Stop when there are no more items that can be classified\n",
    "        items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "        must_get_more_votes = len(items_predicted_classified) > 0\n",
    "    #end while        \n",
    "\n",
    "    items_classification = alg_utils.classify_items(total_votes, gt, params['classification_fn'], ct)\n",
    "\n",
    "    return [items_classification, total_votes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_1():\n",
    "    main_results = []\n",
    "\n",
    "    workers_accuracy = alg_utils.simulate_workers(workers_num, z, fixed_acc, workers_acc)\n",
    "\n",
    "    params = {\n",
    "        'workers_accuracy': workers_accuracy,\n",
    "        'workers_num': workers_num,\n",
    "        'items_num': items_num,\n",
    "        'cost_ratio': cr,\n",
    "        'votes_per_item': base_votes_per_item,\n",
    "        'classification_fn': cf\n",
    "    }\n",
    "\n",
    "    for ct in tqdm(cts):\n",
    "        ct = round(ct, 2) #limit to two decimals\n",
    "        crowd_cost = []\n",
    "        total_cost = []\n",
    "        items_classified_in = []\n",
    "        items_classified_out = []\n",
    "        ct_loss = []\n",
    "        ct_recall = []\n",
    "        ct_precision = []\n",
    "        ct_classified_amount = []\n",
    "        ct_unclassified_amount = []\n",
    "\n",
    "        for _ in range(iterations_per_ct):\n",
    "            ground_truth = alg_utils.generate_gold_data(items_num, data_true_percentage)\n",
    "\n",
    "            ct_i_results = generate_votes(params, items_num, ct, ground_truth)\n",
    "\n",
    "            items_classification = ct_i_results[1]\n",
    "            total_votes = ct_i_results[1]\n",
    "            \n",
    "            classified_amount, unclassified_amount, ct_i_crowd_cost, ct_i_total_cost = alg_utils.get_total_cost(total_votes, cr, cf, ct)\n",
    "            \n",
    "            ct_classified_amount.append(classified_amount)\n",
    "            ct_unclassified_amount.append(unclassified_amount)\n",
    "            crowd_cost.append(ct_i_crowd_cost)\n",
    "            total_cost.append(ct_i_total_cost)\n",
    "\n",
    "            loss,  recall, precision = alg_utils.Metrics.compute_metrics(items_classification, ground_truth)\n",
    "            ct_loss.append(loss)\n",
    "            ct_recall.append(recall)\n",
    "            ct_precision.append(precision)\n",
    "        #end for iterations\n",
    "\n",
    "        main_results.append(\n",
    "            [ct, \n",
    "             round(np.mean(crowd_cost), 3), \n",
    "             round(np.std(crowd_cost), 3),\n",
    "             #round(np.mean(crowd_cost) / items_num, 3), cost per item\n",
    "             round(np.mean(total_cost), 3),\n",
    "             round(np.std(total_cost), 3),\n",
    "             np.mean(classified_amount),\n",
    "             np.mean(unclassified_amount),\n",
    "             np.mean(ct_loss),\n",
    "             np.std(ct_loss),\n",
    "             np.mean(ct_recall),\n",
    "             np.std(ct_recall),\n",
    "             np.mean(ct_precision),\n",
    "             np.std(ct_precision)\n",
    "            ])\n",
    "    #end for thresholds\n",
    "\n",
    "    return main_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "#main \n",
    "cf = majority_voting\n",
    "ct = .9\n",
    "cr = .01 #ratio 1:100\n",
    "base_votes_per_item = 3\n",
    "\n",
    "#cost estimator \n",
    "drawing_simulations_amount = 50\n",
    "expert_cost_increment = 2\n",
    "\n",
    "#crowd\n",
    "workers_num = 10000\n",
    "z = 0 #% cheaters\n",
    "fixed_acc = True\n",
    "workers_acc = .9\n",
    "\n",
    "#ground truth \n",
    "items_num = 100\n",
    "data_true_percentage = 1\n",
    "\n",
    "#experiment \n",
    "cts = np.arange(.7, .96, 0.05) #classification thresholds\n",
    "cts = np.arange(.7, .76, 0.05)\n",
    "iterations_per_ct = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_gold_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c1dd7a9eaa5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"CrowdCost\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"CrowdCost Std\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TC std\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"#IN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"#Expert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Loss Std\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Recall\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Recall Std\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Precision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Precision Std\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpdColumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Threshold\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-32facb3fd4e4>\u001b[0m in \u001b[0;36mrun_experiment_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations_per_ct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_gold_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_true_percentage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mct_i_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_votes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_gold_data' is not defined"
     ]
    }
   ],
   "source": [
    "results_mv = run_experiment_1()\n",
    "columns = [\"CrowdCost\",\"CrowdCost Std\", \"TC\", \"TC std\",\"#IN\",\"#Expert\", \"Loss\", \"Loss Std\", \"Recall\", \"Recall Std\", \"Precision\", \"Precision Std\"]\n",
    "pdColumns = [\"Threshold\"] + columns\n",
    "\n",
    "#plots\n",
    "i = 1\n",
    "for column in columns:\n",
    "    alg_utils.plot_results(cts, {'cost': [x[i] for x in results_mv]}, \"Threshold\", column)\n",
    "    i += 1\n",
    "#table    \n",
    "pd.DataFrame(results_mv, columns=pdColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
