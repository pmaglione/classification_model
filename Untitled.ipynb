{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from helpers.mv_single_binary import majority_voting\n",
    "import helpers.algorithms_utils as alg_utils\n",
    "from helpers.em import expectation_maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Input:\n",
    "        v - votes for item i\n",
    "        ct - value between 0 and 1 for deciding if prob of data is enough or must continue\n",
    "        cf - function to calculate how likely is to be classified\n",
    "        cr - cost ratio between crowd to expert vote [0,1]\n",
    "    Output:\n",
    "        (cost_mean, cost_std)\n",
    "'''\n",
    "def cost_estimator(v, ct, cf, cr):\n",
    "    expert_cost = 1\n",
    "    simulated_costs = []\n",
    "    \n",
    "    for _ in range(drawing_simulations_amount):\n",
    "        must_continue = True\n",
    "        i_item_votes = v.copy()\n",
    "        while (must_continue):\n",
    "            classification_prob = cf(alg_utils.input_adapter_single(i_item_votes))\n",
    "            if classification_prob > ct:\n",
    "                must_continue = False            \n",
    "                simulated_costs.append(len(i_item_votes) * cr)\n",
    "            else:\n",
    "                vote = np.random.binomial(1, classification_prob)\n",
    "                new_index = max(i_item_votes.keys()) + 1\n",
    "                i_item_votes[new_index] = [vote]\n",
    "                #simulated crowd cost is bigger than expert cost\n",
    "                if((len(i_item_votes) * cr) >= (1 * expert_cost_increment)):\n",
    "                    must_continue = False\n",
    "                    simulated_costs.append(len(i_item_votes) * cr)\n",
    "        #end while      \n",
    "    #end for\n",
    "                    \n",
    "    return (np.mean(simulated_costs),np.std(simulated_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to answer: must continue collecting votes over each task?\n",
    "\n",
    "Input:\n",
    "items - set of items\n",
    "votes - set of votes over each item\n",
    "classification_threshold - value between 0 and 1 for deciding if prob of data is enough or must continue\n",
    "cost_ratio - ratio of crowd to expert cost, [0,1]\n",
    "classification_function - function to calculate how likely is to be classified\n",
    "\n",
    "Output:\n",
    "    Dictionary with the decision indexed by item_id\n",
    "        {\n",
    "            item_id: bool\n",
    "            ...\n",
    "            item_n: ...\n",
    "        }\n",
    "    Where False = Stop and True=Continue collecting votes\n",
    "'''\n",
    "def decision_function(items, votes, classification_threshold, cost_ratio, classification_function):      \n",
    "    results = dict.fromkeys(range(items), False)\n",
    "\n",
    "    for item_id, item_true_votes in votes.items():            \n",
    "        item_votes = item_true_votes.copy()\n",
    "        actual_cost = len(item_votes) * cr\n",
    "        classification_prob = classification_function(alg_utils.input_adapter_single(item_votes))\n",
    "        \n",
    "        if ((classification_prob <= classification_threshold) and (actual_cost <= 1)):\n",
    "           \n",
    "            cost_mean, cost_std = cost_estimator(item_votes, classification_threshold, classification_function, cost_ratio)\n",
    "           \n",
    "            if(cost_mean <= 1):\n",
    "                results[item_id] = True\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_votes_1(params, items_num, ct, gt):\n",
    "    total_votes = {}\n",
    "\n",
    "    #base votes\n",
    "    for i in range(items_num):\n",
    "        total_votes[i] = {}\n",
    "        for k in range(params['votes_per_item']):\n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "\n",
    "            total_votes[i][worker_id] = [vote]\n",
    "\n",
    "    #evaluate votes\n",
    "    results = decision_function(items_num, total_votes, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn']) \n",
    "    \n",
    "    #Check if must continue collecting votes\n",
    "    items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "    \n",
    "    must_get_more_votes = len(items_predicted_classified) > 0\n",
    "    \n",
    "    while(must_get_more_votes):\n",
    "        total_votes_aux = {}\n",
    "        for i, v in items_predicted_classified.items():           \n",
    "            worker_id, vote = alg_utils.get_worker_vote(params['workers_accuracy'], i, gt, total_votes)\n",
    "            \n",
    "            total_votes[i][worker_id] = [vote]\n",
    "            \n",
    "            total_votes_aux[i] = total_votes[i]\n",
    "        #end for\n",
    "        results = decision_function(len(total_votes_aux), total_votes_aux, ct, params['cost_ratio'], \n",
    "                                                   params['classification_fn'])\n",
    "\n",
    "        #Stop when there are no more items that can be classified\n",
    "        items_predicted_classified = alg_utils.get_items_predicted_classified(results)\n",
    "        must_get_more_votes = len(items_predicted_classified) > 0\n",
    "    #end while    \n",
    "    \n",
    "    \n",
    "\n",
    "    items_classification = alg_utils.classify_items_mv(total_votes, gt, params['classification_fn'], ct)\n",
    "\n",
    "    return [items_classification, total_votes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_1():\n",
    "    main_results = []\n",
    "\n",
    "    for ct in tqdm(cts):\n",
    "        ct = round(ct, 2) #limit to two decimals\n",
    "        crowd_cost = []\n",
    "        total_cost = []\n",
    "        items_classified_in = []\n",
    "        items_classified_out = []\n",
    "        ct_loss = []\n",
    "        ct_recall = []\n",
    "        ct_precision = []\n",
    "        ct_classified_amount = []\n",
    "        ct_unclassified_amount = []\n",
    "\n",
    "        for _ in range(iterations_per_ct):\n",
    "            workers_accuracy = alg_utils.simulate_workers(workers_num, z, fixed_acc, workers_acc)\n",
    "            \n",
    "            params = {\n",
    "                'workers_accuracy': workers_accuracy,\n",
    "                'workers_num': workers_num,\n",
    "                'items_num': items_num,\n",
    "                'cost_ratio': cr,\n",
    "                'votes_per_item': base_votes_per_item,\n",
    "                'classification_fn': cf\n",
    "            }\n",
    "            \n",
    "            ground_truth = alg_utils.generate_gold_data(items_num, data_true_percentage)\n",
    "\n",
    "            ct_i_results = generate_votes_1(params, items_num, ct, ground_truth)\n",
    "\n",
    "            items_classification = ct_i_results[0]\n",
    "            total_votes = ct_i_results[1]\n",
    "            \n",
    "            classified_amount, unclassified_amount, ct_i_crowd_cost, ct_i_total_cost = alg_utils.get_total_cost(total_votes, cr, cf, ct, False)\n",
    "            \n",
    "            ct_classified_amount.append(classified_amount)\n",
    "            ct_unclassified_amount.append(unclassified_amount)\n",
    "            crowd_cost.append(ct_i_crowd_cost)\n",
    "            total_cost.append(ct_i_total_cost)\n",
    "\n",
    "            loss,  recall, precision = alg_utils.Metrics.compute_metrics(items_classification, ground_truth)\n",
    "            ct_loss.append(loss)\n",
    "            ct_recall.append(recall)\n",
    "            ct_precision.append(precision)\n",
    "        #end for iterations\n",
    "\n",
    "        main_results.append(\n",
    "            [ct, \n",
    "             round(np.mean(crowd_cost), 3), \n",
    "             round(np.std(crowd_cost), 3),\n",
    "             round(np.mean(total_cost), 3),\n",
    "             round(np.std(total_cost), 3),\n",
    "             round(np.mean(ct_loss), 3),\n",
    "             round(np.std(ct_loss), 3),\n",
    "             round(np.mean(ct_recall), 3),\n",
    "             round(np.std(ct_recall), 3),\n",
    "             round(np.mean(ct_precision), 3),\n",
    "             round(np.std(ct_precision), 3)\n",
    "            ])\n",
    "    #end for thresholds\n",
    "\n",
    "    return main_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main \n",
    "cf = majority_voting\n",
    "cr = .01 #ratio 1:10\n",
    "base_votes_per_item = 3\n",
    "\n",
    "#cost estimator \n",
    "drawing_simulations_amount = 50\n",
    "expert_cost_increment = 2\n",
    "\n",
    "#crowd\n",
    "workers_num = 1000\n",
    "z = 0 #% cheaters\n",
    "fixed_acc = False\n",
    "workers_acc = .9\n",
    "\n",
    "#ground truth \n",
    "items_num = 100\n",
    "data_true_percentage = .5\n",
    "\n",
    "#experiment \n",
    "iterations_per_ct = 1\n",
    "cts = [.9] #classification thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:22<00:00, 22.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 22.892268657684326\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "r = run_experiment_1()\n",
    "end = time.time()\n",
    "print(f'Time: {end - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
